{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##by ltecho@whu.edu.cn\n",
    "##\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#域名解析文件路径\n",
    "dns_path = 'C:\\\\bigData\\\\datacon\\\\attack_analysis\\\\extend_data\\\\dns.csv'\n",
    "#部分终端恶意样本文件路径\n",
    "attack_code_path ='C:\\\\bigData\\\\datacon\\\\attack_analysis\\\\extend_data\\\\attacker_code.csv'\n",
    "#绑定域名文件路径\n",
    "bind_domain_path = 'C:\\\\bigData\\\\datacon\\\\attack_analysis\\\\extend_data\\\\domain_info.csv'\n",
    "#md5对应恶意样本名文件路径\n",
    "attack_file_path = 'C:\\\\bigData\\\\datacon\\\\attack_analysis\\\\extend_data\\\\attack_code_file.json'\n",
    "##域名类别文件\n",
    "domain_category_path = 'C:\\\\bigData\\\\datacon\\\\attack_analysis\\\\extend_data\\\\domain_category.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取日志中存在的ip，作为其他数据筛选的依据\n",
    "ip_list=pd.read_csv('ip_file\\\\result.csv',delimiter=',',encoding='utf-8',low_memory=False)\n",
    "ip_list.rename(columns={'file_id':'ip'},inplace=True)\n",
    "ip_list['in_logs']=ip_list['is_attacker'].apply(lambda x:1)\n",
    "ip_list.drop(columns=['reason','is_attacker'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入毫秒级的时间，转出正常格式的时间\n",
    "def timeStamp(timeNum):\n",
    "    timeStamp = float(timeNum/1000)\n",
    "    timeArray = time.localtime(timeStamp)\n",
    "    otherStyleTime = time.strftime(\"%Y.%m.%d\", timeArray)\n",
    "    return otherStyleTime\n",
    "\n",
    "\n",
    "#获得日志中ip对应的dns解析数据\n",
    "#处理域名解析DNS.csv文件，去除冗余数据，按IP，date，domain_name分组，计数解析次数，存储为dns_info_result.csv文件\n",
    "def get_dns_result():\n",
    "    dns_cloumns = ['ip','date','domain_name','resolve_date']\n",
    "    dns_infos = pd.read_csv(dns_path,delimiter=',',names=dns_cloumns,encoding='utf-8',dtype='str')\n",
    "    dns_infos.duplicated().drop_duplicates(inplace=True)\n",
    "    dns_infos_result=dns_infos.groupby(['ip','date','domain_name']).count()\n",
    "    dns_infos_result.reset_index(inplace=True)\n",
    "    dns_infos_result.rename(columns={'resolve_date':'resolve_number'},inplace=True)\n",
    "    dns_infos_result.to_csv('ip_file\\\\dns_infos_result.csv',index=0)\n",
    "    \n",
    "#处理域名解析dns.csv预处理后的dns_infos_result.csv。筛选出攻击ip列表的解析数据，\n",
    "##处理域名类别文件，将解析域名与类别关联，得到（ip，domain_name,domain_category）关联数据\n",
    "def get_ip_log_dns():\n",
    "    dns_infos_result=pd.read_csv('ip_file\\\\dns_infos_result.csv',delimiter=',',encoding='utf-8',low_memory=False)\n",
    "    dns_infos_result[dns_infos_result.isnull().values==True].drop_duplicates()\n",
    "    ip_log_dns_info = pd.merge(dns_infos_result,ip_list,on='ip',how='left')\n",
    "    ip_log_dns_info[ip_log_dns_info.isnull().values==True].drop_duplicates()\n",
    "    ip_log_dns_info.dropna(axis=0,how='any',inplace=True)\n",
    "    #删除没有意义后的in_logs标识\n",
    "    ip_log_dns_info.drop('in_logs',axis=1,inplace=True)\n",
    "    ip_log_dns_info.to_csv('ip_file\\\\ip_log_dns_info.csv',index=0)\n",
    "    #将域名和域名类别关联\n",
    "    with open(domain_category_path,'r') as load_f:\n",
    "        domain_category_dict = json.load(load_f)\n",
    "    domain_category_list=[]\n",
    "    for k,v in domain_category_dict.items():\n",
    "        domain_category={}\n",
    "        domain_category['domain_category']=v\n",
    "        domain_category['domain_name']=k\n",
    "        domain_category_list.append(domain_category)\n",
    "    domain_category_list = pd.DataFrame(domain_category_list)\n",
    "    ip_log_dns_cate = pd.merge(ip_log_dns_info,domain_category_list,on='domain_name',how='left')\n",
    "    ip_log_dns_cate.to_csv('ip_file\\\\ip_log_dns_cate.csv',index=0)\n",
    "\n",
    "\n",
    "##处理部分终端恶意文件，得到属于攻击ip列表的数据\n",
    "##关联恶意文件md5和恶意文件名，得到ip与恶意文件的对应关系\n",
    "def get_ip_attack_code():\n",
    "    attack_code_columns = ['ip','code_md5','mid','date']\n",
    "    attack_code = pd.read_csv(attack_code_path,delimiter=',',names=attack_code_columns,encoding='utf-8')\n",
    "    attack_code[attack_code.isnull().values==True].drop_duplicates()\n",
    "    attack_code.fillna(0,inplace=True)\n",
    "    ip_attack_code = pd.merge(attack_code,ip_list,on='ip',how='left')\n",
    "    ip_attack_code[ip_attack_code.isnull().values==True].drop_duplicates()\n",
    "    ip_attack_code.dropna(axis=0,how='any',inplace=True)\n",
    "    #删除没有意义后的in_logs标识\n",
    "    ip_attack_code.drop('in_logs',axis=1,inplace=True)\n",
    "    ip_attack_code.to_csv('ip_file\\\\ip_attack_code.csv',index=0)\n",
    "    \n",
    "    #关联md5和恶意文件名\n",
    "    with open(attack_file_path,'r') as load_f:\n",
    "        attack_code_file_dict = json.load(load_f)\n",
    "    attack_md5_file=[]\n",
    "    for k,v in attack_code_file_dict.items():\n",
    "        md5_file={}\n",
    "        md5_file['code_md5']=k\n",
    "        md5_file['file_name']=v\n",
    "        attack_md5_file.append(md5_file)\n",
    "    attack_md5_file = pd.DataFrame(attack_md5_file)\n",
    "    ip_attack_code = ip_attack_code.reset_index()\n",
    "    ip_attack_file = pd.merge(ip_attack_code,attack_md5_file,on='code_md5',how='left')\n",
    "    #将时间戳日期格式化为年月日，便于删除同一天的冗余恶意样本数据，\n",
    "    ip_attack_file['date']=ip_attack_file['date'].apply(lambda x:timeStamp(x))\n",
    "    ip_attack_file= ip_attack_file.astype('str')\n",
    "    #删除冗余数据\n",
    "    ip_attack_file.drop_duplicates(inplace=True)\n",
    "    ip_attack_file.to_csv('ip_file\\\\ip_attack_file.csv',index=0)\n",
    "\n",
    "\n",
    "#处理ip绑定域名文件，得到属于攻击ip列表的相关数据\n",
    "def get_ip_domain():\n",
    "    domain_cloumns = ['ip','date','situation','whois']\n",
    "    domain_infos = pd.read_csv(bind_domain_path,delimiter=',',names=domain_cloumns,encoding='utf-8',dtype='str')\n",
    "    domain_infos.drop_duplicates(inplace=True)\n",
    "    ip_log_domain_infos = pd.merge(domain_infos,ip_list,on='ip',how='left')\n",
    "    ip_log_domain_infos.dropna(axis=0,how='any',inplace=True)\n",
    "    ip_log_domain_infos.drop('in_logs',axis=1,inplace=True)\n",
    "    ip_log_domain_infos.to_csv('ip_file\\\\ip_log_domain.csv',index=0)\n",
    "##将所有域名提炼出来\n",
    "def get_domain_info():\n",
    "    domain_infos_have = pd.read_csv('ip_file\\\\ip_log_domain.csv',delimiter=',',encoding='utf-8')\n",
    "    domain_columns = []\n",
    "    domain_info_list = []\n",
    "    for index,domain_info in domain_infos_have.iterrows():\n",
    "        if domain_info.whois !='[]':\n",
    "            #domain_whois = domain_info.whois\n",
    "            domain_whois_dict=eval(domain_info.whois)\n",
    "            #print(domain_info.whois)\n",
    "            domain_list=domain_whois_dict[0]['domainWhois']\n",
    "            domain_info_list = domain_info_list + domain_list\n",
    "    domain_list_data = pd.DataFrame(domain_info_list)\n",
    "    domain_list_data.rename(columns=lambda x:x.strip().lower().replace(' ',\"_\"),inplace=True)\n",
    "    domain_list_data= domain_list_data.astype('str')\n",
    "    domain_list_data.drop_duplicates(inplace=True)\n",
    "    domain_list_data.to_csv('ip_file\\domain_list_result.csv',index=0)\n",
    "    \n",
    "##将ip与域名的一对多关系数据进行处理，得到ip与域名的一对一关系数据\n",
    "def get_ip_domain_relation():\n",
    "    domain_infos_have = pd.read_csv('ip_file\\\\ip_log_domain.csv',delimiter=',',encoding='utf-8')\n",
    "    domain_columns = []\n",
    "    ip_domain_list = []\n",
    "    for index,domain_info in domain_infos_have.iterrows():\n",
    "        #domain_whois = domain_info.whois\n",
    "        if domain_info.situation !='[]':\n",
    "            situation_dict=eval(domain_info.situation)\n",
    "            #print(domain_info.whois)\n",
    "            #domain_list=domain_whois_dict[0]['domainWhois']\n",
    "            for i in range(len(situation_dict)):\n",
    "                #源ip\n",
    "                situation_dict[i]['source_ip'] = domain_info.ip\n",
    "                situation_dict[i]['date'] = domain_info.date\n",
    "            ip_domain_list = ip_domain_list + situation_dict\n",
    "    ip_domain_data = pd.DataFrame(ip_domain_list)\n",
    "    ip_domain_data.rename(columns=lambda x:x.strip().lower().replace(' ',\"_\"),inplace=True)\n",
    "    #已经有date，所以将endtime和starttime删除，去除冗余记录\n",
    "    ip_domain_data.drop(columns=['endtime','starttime'],axis=1,inplace=True)\n",
    "    ip_domain_data['ip'] = ip_domain_data['ip'].apply(lambda x:x[:-1])\n",
    "    ip_domain_data.drop_duplicates(inplace=True)\n",
    "    #保存ip_dimain关系的记录数据\n",
    "    ip_domain_data.to_csv('ip_file\\\\ip_domain.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dns_result()\n",
    "get_ip_log_dns()\n",
    "get_ip_attack_code()\n",
    "get_ip_domain()\n",
    "get_domain_info()\n",
    "get_ip_domain_relation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
